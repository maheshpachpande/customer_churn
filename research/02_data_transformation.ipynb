{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\pachp\\\\Desktop\\\\projects\\\\customer_churn\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\pachp\\\\Desktop\\\\projects\\\\customer_churn'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    preprocessor_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_churn.constants import *\n",
    "from customer_churn.utils.common_utils import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            preprocessor_path=config.preprocessor_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from box.exceptions import BoxValueError\n",
    "from customer_churn import logger\n",
    "from customer_churn.utils.common_utils import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "\n",
    "        '''\n",
    "        This is responcible for data transformation\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "\n",
    "            num_features = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'Churn Score',\n",
    "       'CLTV']\n",
    "            \n",
    "            cat_features = ['City', 'Gender', 'Senior Citizen', 'Partner', 'Dependents',\n",
    "       'Phone Service', 'Multiple Lines', 'Internet Service',\n",
    "       'Online Security', 'Online Backup', 'Device Protection', 'Tech Support',\n",
    "       'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing',\n",
    "       'Payment Method']\n",
    "            \n",
    "            logger.info(\"Pipeline initiated.....\")\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\",SimpleImputer()),\n",
    "                    (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"onehot\", OneHotEncoder(sparse_output=False, drop=\"first\",dtype=np.int16))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logger.info(\"Pipeline completed.\")\n",
    "\n",
    "            logger.info(\"columntransformation initiated.....\")\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"Numerical Pipeline\", num_pipeline, num_features),\n",
    "                    (\"Categorical Pipeline\", cat_pipeline, cat_features)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logger.info(\"columntransformation completed.\")\n",
    "\n",
    "            return preprocessor\n",
    "        except BoxValueError:\n",
    "            raise ValueError(\"Error occured at data tranformation.....\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "        \n",
    "    def initiate_data_transformation(self, train_set, test_set):\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_set)\n",
    "            test_df = pd.read_csv(test_set)\n",
    "            logger.info(\"Read train and test data completed.\")\n",
    "\n",
    "            preprocessor_obj = self.get_data_transformer_object()\n",
    "\n",
    "            target_column_name = \"Churn Value\"\n",
    "\n",
    "            logger.info(\"separate the independent and dependent columns started\")\n",
    "            \n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            logger.info(\"separate the independent and dependent columns completed\")\n",
    "\n",
    "            logger.info(\"Applying preprocessing object on train and test dataset\")\n",
    "\n",
    "            input_feature_train_arr = preprocessor_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessor_obj.transform(input_feature_test_df)\n",
    "\n",
    "            train_arr = np.c_[\n",
    "                input_feature_train_arr, np.array(target_feature_train_df)\n",
    "            ]\n",
    "\n",
    "            test_arr = np.c_[\n",
    "                input_feature_test_arr, np.array(target_feature_test_df)\n",
    "            ]\n",
    "\n",
    "            logger.info(\"Saved preprocessing object.\")\n",
    "\n",
    "            \n",
    "\n",
    "            save_object(\n",
    "                file_path = self.config.preprocessor_path,\n",
    "                obj = preprocessor_obj\n",
    "            )\n",
    "            np.save('train_arr.npy',train_arr)\n",
    "\n",
    "            return (\n",
    "                \n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                self.config.preprocessor_path\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        except BoxValueError:\n",
    "            raise ValueError(\"Error occured at initiate data transformation.....\")\n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-14 00:33:22,642: INFO: common_utils: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-03-14 00:33:22,648: INFO: common_utils: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-14 00:33:22,651: INFO: common_utils: created directory at: artifacts]\n",
      "[2024-03-14 00:33:22,657: INFO: common_utils: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-14 00:33:22,771: INFO: 2983784864: Read train and test data completed.]\n",
      "[2024-03-14 00:33:22,771: INFO: 2983784864: Pipeline initiated.....]\n",
      "[2024-03-14 00:33:22,771: INFO: 2983784864: Pipeline completed.]\n",
      "[2024-03-14 00:33:22,771: INFO: 2983784864: columntransformation initiated.....]\n",
      "[2024-03-14 00:33:22,786: INFO: 2983784864: columntransformation completed.]\n",
      "[2024-03-14 00:33:22,786: INFO: 2983784864: separate the independent and dependent columns started]\n",
      "[2024-03-14 00:33:22,786: INFO: 2983784864: separate the independent and dependent columns completed]\n",
      "[2024-03-14 00:33:22,786: INFO: 2983784864: Applying preprocessing object on train and test dataset]\n",
      "[2024-03-14 00:33:23,011: INFO: 2983784864: Saved preprocessing object.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()   \n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train_arr, test_arr, _ = data_transformation.initiate_data_transformation(\"artifacts/data_ingestion/train.csv\",\"artifacts/data_ingestion/test.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_arr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_arr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     yaml\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pachp\\Desktop\\projects\\customer_churn\\cust\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_arr'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"train_arr\",\"r\") as f:\n",
    "    yaml.dump(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5634, 1161)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_arr)\n",
    "df.to_csv(\"artifacts/df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_arr)\n",
    "df.to_csv(\"artifacts/df1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.57088583,  1.13316589,  2.07084713, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.669405  ,  0.95270761, -0.31009531, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.01768403,  1.3119686 ,  0.50421382, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.54720732, -1.46444011, -0.83420617, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.10451365, -1.4743736 , -0.71111986, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.05841659,  1.29210163,  0.34940606, ...,  1.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
